{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "df = [ts: string, number: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ts: string, number: string ... 4 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\", \"true\").csv(\"/tmp/nithin/3p/dump/ct_rr.csv\").cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----------+-----------------+---------+-----------------+\n",
      "|                 ts|number|  pick_lat|         pick_lng| drop_lat|         drop_lng|\n",
      "+-------------------+------+----------+-----------------+---------+-----------------+\n",
      "|2018-04-07 07:07:17| 14626|12.3136215|76.65819499999998|12.287301|76.60228000000002|\n",
      "|2018-04-07 07:32:27| 85490| 12.943947|        77.560745|12.954014|         77.54377|\n",
      "+-------------------+------+----------+-----------------+---------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object AggType\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "object AggType extends Enumeration {\n",
    "    type AggType = Value  \n",
    "    protected case class Val(aggTypeName: String) extends super.Val {}\n",
    "\n",
    "    implicit def valueToFormatVal(x: Value): Val = x.asInstanceOf[Val]\n",
    "\n",
    "    val hourly = Val(\"hourly\")\n",
    "    val daily = Val(\"daily\")\n",
    "    val monthly = Val(\"monthly\")\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_agg_stats: (df: org.apache.spark.sql.DataFrame, startDate: java.time.LocalDate, endDate: java.time.LocalDate)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import java.time.LocalDate\n",
    "def avg_agg_stats(df: DataFrame, startDate: LocalDate, endDate: LocalDate) = {\n",
    "\n",
    "    val tmp = df\n",
    "        .filter('ts.between(startDate.toString, endDate.toString))\n",
    "        .withColumn(\"date\", to_date('ts))\n",
    "        .withColumn(\"month\", month('ts))\n",
    "        .withColumn(\"hour\", hour('ts))\n",
    "        .cache\n",
    "    \n",
    "    val dailyAggDf = tmp.groupBy('number, 'date).agg(count(\"*\").as(\"daily_cnt\"))\n",
    "    val hourlyAggDf = tmp.groupBy('number, 'hour).agg(count(\"*\").as(\"hourly_cnt\"))\n",
    "    val monthlyAggDf = tmp.groupBy('number, 'month).agg(count(\"*\").as(\"monthly_cnt\"))\n",
    "    \n",
    "    \n",
    "    val avg_daily = dailyAggDf.select(avg('daily_cnt)).head().getAs[Double](0)\n",
    "    val avg_hourly = hourlyAggDf.select(avg('hourly_cnt)).head().getAs[Double](0)\n",
    "    val avg_monthly = monthlyAggDf.select(avg('monthly_cnt)).head().getAs[Double](0)\n",
    "    \n",
    "    \n",
    "    val res = s\"Avg. of aggregated counts:\\n\\t * hourly: ${avg_hourly}\\n\\t * daily: ${avg_daily}\\n\\t * monthly: ${avg_monthly}\"\n",
    "    println(res)\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. of aggregated counts:\n",
      "\t * hourly: 2.392940542482486\n",
      "\t * daily: 3.0919113380526864\n",
      "\t * monthly: 3.973009245451834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "startDate = 2018-04-01\n",
       "endDate = 2018-04-10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2018-04-10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val startDate = LocalDate.parse(\"2018-04-01\")\n",
    "val endDate = LocalDate.parse(\"2018-04-10\")\n",
    "avg_agg_stats(df, startDate, endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tmp = [ts: string, number: string ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ts: string, number: string ... 7 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tmp = df\n",
    "\n",
    "        .withColumn(\"date\", to_date('ts))\n",
    "        .withColumn(\"month\", month('ts))\n",
    "        .withColumn(\"hour\", hour('ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dailyAggDf = [number: string, date: date ... 1 more field]\n",
       "hourlyAggDf = [number: string, hour: int ... 1 more field]\n",
       "monthlyAggDf = [number: string, month: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3.3086164774039366"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dailyAggDf = tmp.groupBy('number, 'date).agg(count(\"*\").as(\"daily_cnt\"))\n",
    "val hourlyAggDf = tmp.groupBy('number, 'hour).agg(count(\"*\").as(\"hourly_cnt\"))\n",
    "val monthlyAggDf = tmp.groupBy('number, 'month).agg(count(\"*\").as(\"monthly_cnt\"))\n",
    "\n",
    "\n",
    "dailyAggDf.select(avg('daily_cnt)).head().getAs[Double](0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startDate = 2018-04-01\n",
       "noOfWeeks = 3\n",
       "endDate = 2018-04-22\n",
       "filteredDf = [number: string, week: int]\n",
       "minWeek = 14\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.time.LocalDate\n",
    "val startDate = LocalDate.parse(\"2018-04-01\")\n",
    "val noOfWeeks = 3\n",
    "val endDate = startDate.plusDays(noOfWeeks * 7)\n",
    "\n",
    "val filteredDf = df.filter('ts.between(lit(startDate.toString), lit(endDate.toString)))\n",
    "    .withColumn(\"week\", weekofyear('ts))\n",
    "    .dropDuplicates(\"number\", \"week\")\n",
    "    .select('number, 'week)\n",
    "    .cache\n",
    "\n",
    "val minWeek = filteredDf.select(min('week)).head().getAs[Int](0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+\n",
      "|number|week|first_week|\n",
      "+------+----+----------+\n",
      "|  None|  14|        14|\n",
      "|  None|  15|        14|\n",
      "|  None|  16|        14|\n",
      "| 00004|  15|        15|\n",
      "| 00005|  16|        16|\n",
      "| 00007|  16|        16|\n",
      "| 00009|  14|        14|\n",
      "| 00009|  15|        14|\n",
      "| 00010|  14|        14|\n",
      "| 00013|  16|        16|\n",
      "| 00017|  14|        14|\n",
      "| 00025|  16|        16|\n",
      "| 00029|  15|        15|\n",
      "| 00030|  16|        16|\n",
      "| 00040|  15|        15|\n",
      "| 00040|  16|        15|\n",
      "| 00042|  14|        14|\n",
      "| 00042|  16|        14|\n",
      "| 00053|  14|        14|\n",
      "| 00053|  15|        14|\n",
      "+------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "retentionWindow = org.apache.spark.sql.expressions.WindowSpec@7ccde36b\n",
       "ret = [number: string, week: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[number: string, week: int ... 1 more field]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val retentionWindow = Window.partitionBy('number).orderBy('week.asc)\n",
    "val ret = filteredDf\n",
    "    .withColumn(\"first_week\", min('week).over(retentionWindow))\n",
    "    //.withColumn(\"retention_week\", 'week - 'first_week)\n",
    "    .orderBy('number, 'week)\n",
    "\n",
    "ret.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---------+\n",
      "|first_week|week|tot_users|\n",
      "+----------+----+---------+\n",
      "|        14|  14|     4345|\n",
      "|        14|  15|     2718|\n",
      "|        14|  16|     2312|\n",
      "|        15|  15|     8848|\n",
      "|        15|  16|     3971|\n",
      "|        16|  16|     4513|\n",
      "+----------+----+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dist = [first_week: int, week: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[first_week: int, week: int ... 1 more field]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dist = ret.groupBy('first_week, 'week).agg(count(\"number\").as(\"tot_users\"))\n",
    ".orderBy('first_week, 'week)\n",
    "dist.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+\n",
      "|first_week|  14|  15|  16|\n",
      "+----------+----+----+----+\n",
      "|        14|4345|2718|2312|\n",
      "|        15|null|8848|3971|\n",
      "|        16|null|null|4513|\n",
      "+----------+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dist.groupBy('first_week).pivot('week).agg(first('tot_users))\n",
    "    .orderBy('first_week)\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "firstWeek = [number: string, first_week: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "17706"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val firstWeek = filteredDf\n",
    "    .withColumn(\"week\", weekofyear('ts))\n",
    "    .groupBy('number).agg(min('week).as(\"first_week\"))\n",
    "firstWeek.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_week|new_users|\n",
      "+----------+---------+\n",
      "|        16|     4513|\n",
      "|        15|     8848|\n",
      "|        14|     4345|\n",
      "+----------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newUsers = [first_week: int, new_users: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[first_week: int, new_users: bigint]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newUsers = firstWeek.groupBy('first_week).agg(count('number).as(\"new_users\"))\n",
    "newUsers.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cohort: (df: org.apache.spark.sql.DataFrame, startDate: java.time.LocalDate, noOfWeeks: Int)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "  def cohort(df: DataFrame, startDate: LocalDate, noOfWeeks: Int): DataFrame = {\n",
    "    val endDate = startDate.plusWeeks(noOfWeeks)\n",
    "\n",
    "    val filteredDf = df.filter('ts.between(lit(startDate.toString), lit(endDate.toString)))\n",
    "      .withColumn(\"week\", weekofyear('ts))\n",
    "      .dropDuplicates(\"number\", \"week\")\n",
    "      .select('number, 'week)\n",
    "      .cache\n",
    "\n",
    "    val minWeek = filteredDf.select(min('week)).head().getAs[Int](0)\n",
    "    val weekRange = (minWeek until minWeek + noOfWeeks+1)\n",
    "    \n",
    "    val retentionWindow = Window.partitionBy('number).orderBy('week.asc)\n",
    "\n",
    "    val distribution = filteredDf\n",
    "      .withColumn(\"first_week\", min('week).over(retentionWindow))\n",
    "      .groupBy('first_week, 'week)\n",
    "      .agg(count('number).as(\"tot_users\"))\n",
    "\n",
    "    val result = distribution.groupBy('first_week)\n",
    "      .pivot('week, weekRange).agg(first('tot_users))\n",
    "      .orderBy('first_week)\n",
    "\n",
    "    result\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-----+-----+\n",
      "|first_week|2    |3    |4    |5    |6    |7    |\n",
      "+----------+-----+-----+-----+-----+-----+-----+\n",
      "|2         |12997|9473 |9038 |8946 |9084 |8286 |\n",
      "|3         |null |22454|13311|13014|13406|12180|\n",
      "|4         |null |null |11651|5437 |5576 |4848 |\n",
      "|5         |null |null |null |7848 |3547 |3039 |\n",
      "|6         |null |null |null |null |6679 |2556 |\n",
      "|7         |null |null |null |null |null |4397 |\n",
      "+----------+-----+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "st = 2019-01-12\n",
       "t = [first_week: int, 2: bigint ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[first_week: int, 2: bigint ... 5 more fields]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val st = LocalDate.parse(\"2019-01-12\")\n",
    "val t = cohort(df, st, 5)\n",
    "t.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+\n",
      "|first_week|  14|  15|  16|\n",
      "+----------+----+----+----+\n",
      "|        14|4345|2718|2312|\n",
      "|        15|null|8848|3971|\n",
      "|        16|null|null|4513|\n",
      "+----------+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cohort(df, LocalDate.parse(\"2018-04-01\"), 3).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f: [T](v: T)T\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
